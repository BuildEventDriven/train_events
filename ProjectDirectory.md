# Project Directory Structure

This repository contains a well-structured directory for managing Kafka-based AI/ML workflows. Below is the organized directory structure with descriptions.

```
ðŸ“¦ project-root
â”œâ”€â”€ ðŸ“‚ infra                   # Infrastructure setup scripts
â”‚   â”œâ”€â”€ ðŸ“„ instance-creation.sh   # Script to create compute instances
â”‚   â”œâ”€â”€ ðŸ“„ kafka-setup.sh         # Kafka cluster setup script
â”‚   â”œâ”€â”€ ðŸ“„ zookeeper-setup.sh     # Zookeeper setup script
â”‚   â”œâ”€â”€ ðŸ“„ monitoring-setup.sh    # Monitoring setup script
â”œâ”€â”€ ðŸ“‚ terraform-scripts        # Terraform scripts for provisioning cloud resources
â”œâ”€â”€ ðŸ“‚ kubernetes-manifests     # Kubernetes YAML manifests for deployments
â”œâ”€â”€ ðŸ“‚ kafka                   # Kafka producer/consumer scripts and configurations
â”‚   â”œâ”€â”€ ðŸ“„ producer.py            # Kafka producer script
â”‚   â”œâ”€â”€ ðŸ“„ consumer.py            # Kafka consumer script
â”‚   â”œâ”€â”€ ðŸ“„ topics-config.json     # Kafka topic configurations
â”œâ”€â”€ ðŸ“‚ schema-registry          # Schema definitions for Kafka message serialization
â”œâ”€â”€ ðŸ“‚ ingestion                # Data ingestion pipelines
â”‚   â”œâ”€â”€ ðŸ“„ flink-data-quality.py  # Flink script for data quality checks
â”‚   â”œâ”€â”€ ðŸ“„ spark-batch-ingestion.py # Spark batch ingestion script
â”œâ”€â”€ ðŸ“‚ kafka-connect-config     # Configuration for Kafka Connect integration
â”œâ”€â”€ ðŸ“‚ model-training           # Machine learning model training pipeline
â”‚   â”œâ”€â”€ ðŸ“„ train_model.py        # ML model training script
â”‚   â”œâ”€â”€ ðŸ“„ preprocess_data.py    # Data preprocessing script
â”œâ”€â”€ ðŸ“‚ mlflow-tracking          # MLflow tracking for experiments and logging
â”œâ”€â”€ ðŸ“‚ inference                # Inference pipelines for real-time and batch predictions
â”‚   â”œâ”€â”€ ðŸ“„ real-time-inference.py  # Real-time inference script
â”‚   â”œâ”€â”€ ðŸ“„ batch-inference.py     # Batch inference script
â”œâ”€â”€ ðŸ“‚ grpc-serving             # gRPC-based model serving setup
â”œâ”€â”€ ðŸ“‚ monitoring               # Monitoring configurations and dashboards
â”‚   â”œâ”€â”€ ðŸ“„ prometheus-config.yml  # Prometheus configuration
â”‚   â”œâ”€â”€ ðŸ“„ grafana-dashboard.json # Grafana dashboard setup
â”‚   â”œâ”€â”€ ðŸ“„ whylogs-monitoring.py  # WhyLogs monitoring script for ML models
â”œâ”€â”€ ðŸ“‚ ci-cd                    # CI/CD pipeline setup
â”‚   â”œâ”€â”€ ðŸ“‚ github-actions       # GitHub Actions workflows
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ ci-cd-pipeline.yml  # CI/CD workflow for GitHub Actions
â”‚   â”œâ”€â”€ ðŸ“‚ argo-cd             # ArgoCD GitOps configuration
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ deployment.yaml    # ArgoCD deployment configuration
â”‚   â”œâ”€â”€ ðŸ“‚ kubeflow-pipelines  # Kubeflow pipelines for ML workflows
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ pipeline.yaml      # Kubeflow pipeline definition
â”œâ”€â”€ ðŸ“‚ docs                     # Documentation and guides
â”‚   â”œâ”€â”€ ðŸ“„ architecture-diagram.png  # System architecture diagram
â”‚   â”œâ”€â”€ ðŸ“„ README.md                 # Project README
â”‚   â”œâ”€â”€ ðŸ“„ setup-guide.md             # Setup instructions
â”œâ”€â”€ ðŸ“‚ scripts                  # Utility scripts
â”‚   â”œâ”€â”€ ðŸ“„ data-simulator.py       # Script to simulate data
â”‚   â”œâ”€â”€ ðŸ“„ benchmark-tests.py      # Performance benchmarking script
â”œâ”€â”€ ðŸ“‚ terraform-scripts        # Terraform scripts for cloud infrastructure
â”‚   â”œâ”€â”€ ðŸ“„ main.tf              # Defines VM, networking, storage
â”‚   â”œâ”€â”€ ðŸ“„ variables.tf         # Configurable Terraform variables
â”‚   â”œâ”€â”€ ðŸ“„ outputs.tf           # Outputs from Terraform execution
â”‚   â”œâ”€â”€ ðŸ“„ terraform.tfvars     # Default values for variables
â”‚   â”œâ”€â”€ ðŸ“„ provider.tf          # Google Cloud provider setup
â”œâ”€â”€ ðŸ“‚ .github/workflows        # GitHub Actions workflow directory
â”‚   â”œâ”€â”€ ðŸ“„ ci-cd-pipeline.yml      # Workflow for CI/CD
â”œâ”€â”€ ðŸ“„ .gitignore                # Files and directories to be ignored by Git
â”œâ”€â”€ ðŸ“„ README.md                 # Main project README
```

## Description
Each directory serves a specific purpose:

- **infra**: Scripts for setting up Kafka, Zookeeper, and monitoring.
- **terraform-scripts**: Terraform infrastructure as code (IaC) configurations.
- **kubernetes-manifests**: YAML files for Kubernetes deployments.
- **kafka**: Producer and consumer implementations for event streaming.
- **schema-registry**: Manages Avro schemas for message serialization.
- **ingestion**: Flink and Spark-based data ingestion pipelines.
- **kafka-connect-config**: Configurations for Kafka Connect integrations.
- **model-training**: ML model training and data preprocessing.
- **mlflow-tracking**: Experiment tracking using MLflow.
- **inference**: Real-time and batch inference scripts.
- **grpc-serving**: gRPC-based model serving infrastructure.
- **monitoring**: Prometheus, Grafana, and WhyLogs configurations.
- **ci-cd**: Continuous integration and deployment workflows.
- **docs**: Documentation and guides for system setup.
- **scripts**: Utility scripts for data simulation and benchmarking.

This structure ensures a modular, scalable, and maintainable approach to managing Kafka-based AI/ML workflows.
